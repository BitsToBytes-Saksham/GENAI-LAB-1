{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 1 – Model Benchmark Challenge\n",
        "\n",
        "## Objective\n",
        "The goal of this notebook is to compare the behavior of three transformer architectures:\n",
        "\n",
        "- **BERT (Encoder-only)**\n",
        "- **RoBERTa (Encoder-only)**\n",
        "- **BART (Encoder–Decoder)**\n",
        "\n",
        "Each model is deliberately forced to perform tasks it may not be architecturally suited for, in order to observe failures and understand why architecture matters.\n"
      ],
      "metadata": {
        "id": "EjRYHHNdN1wk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phK-b3vt_aZC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Ia2h8J_nix",
        "outputId": "fbc7d524-9ef1-4efa-d37d-6b67d4733fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models Used\n",
        "\n",
        "| Model | Hugging Face Identifier | Architecture |\n",
        "|------|------------------------|--------------|\n",
        "| BERT | bert-base-uncased | Encoder-only |\n",
        "| RoBERTa | roberta-base | Encoder-only |\n",
        "| BART | facebook/bart-base | Encoder–Decoder |"
      ],
      "metadata": {
        "id": "LYgxEyEbOLUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}"
      ],
      "metadata": {
        "id": "eC02FpHUOR7T"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1: Text Generation\n",
        "\n",
        "### Task\n",
        "Generate text given the prompt:\n",
        "\n",
        "> **\"The future of Artificial Intelligence is\"**\n",
        "\n",
        "### Expected Behavior\n",
        "- Encoder-only models (BERT, RoBERTa) should fail or behave poorly because they are not trained for autoregressive text generation.\n",
        "- BART, which has a decoder, should generate longer text, though quality may vary.\n"
      ],
      "metadata": {
        "id": "cTJWQQ5_OY35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        generator = pipeline(\"text-generation\", model=model)\n",
        "        output = generator(prompt, max_length=30)\n",
        "        print(output)\n",
        "    except Exception as e:\n",
        "        print(\"FAILED:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scIuaua5_0aE",
        "outputId": "a4d3b6ae-a0d9-4061-ab72-ece24a9d5e4b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: BERT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
            "\n",
            "Model: RoBERTa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence is'}]\n",
            "\n",
            "Model: BART\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence is Ji Ji Marriott Ji Ji Ji PHumentbj Ji torrent torrent bent packed Ji Jiacteria disdain Marriott bent playoff TPPacters Marriott Marriottisconsin bentisconsin bent bent Ji Cesisconsin Ji Jiisconsinisconsinisconsin arsenicBAT Prohibition OCT OCT OCT Pyramid rollout bent OCT Prohibition------------- Troll spam spam OCT wells Ji Ji spam OCT OCT Ji fraternityacteria Ji fraternity Jiacteria OCT spam OCT solitude exceedingly Ji exceedingly bent Sunder Ji sperm Ji Ji fraternity injected Ji Ji phosphate Sunder spam provoked injected Combat fraternity injected injected spam injected spam sperm OCT OCT provoked OCT OCTichick provokedobal Sunder Ji Ji Sunder arsenic Ji Prohibition 275 OCT erotic OCT OCT erotic spam OCTatche OCT OCTcas provoked OCT Prohibition solitude spam erotic OCT ISPs spam OCT provoked provoked Judicial spam spam Ji provoked Sunder Sunder sperm Subaru provoked Ji Sunder 275cas OCT spam spam bent OCT Sunder OCT OCT Martha bent spam provoked provoked erotic dysfunction Sunder spam spam spam provoked spam Sunder bent OCT OCT solitude OCT injected playoff injected erotic Ji spam injected Sunder provoked injected spam spamobal provoked spam OCT spam provoked OCT Sunder Morty OCT spam Martha injected spam advised provoked spam provoked bent bent provoked OCT Benefit OCT provoked dysfunction provoked OCT spam injected OCT OCT bent advised spam spamnin spam erotic injected spam OCT injected provokedacters injected spamcascas OCT OCT spam dysfunction provoked injectedRating provoked glim provokedacters spamass spam spamawaru spam'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations (Experiment 1 – Text Generation)\n",
        "\n",
        "- **BERT** generated a sequence consisting almost entirely of repeated punctuation (dots) and failed to produce any meaningful continuation of the prompt.\n",
        "- **RoBERTa** simply repeated the original prompt without generating any additional text.\n",
        "- **BART** generated a long continuation of the prompt; however, the output was highly noisy and incoherent, containing unrelated words and repetitive patterns.\n",
        "\n",
        "### Explanation\n",
        "\n",
        "BERT and RoBERTa are encoder-only transformer models. They are trained for language understanding tasks such as masked language modeling and classification, not for predicting the next token in a sequence. As a result, they are unable to perform autoregressive text generation, leading to repetitive or empty outputs.\n",
        "\n",
        "BART uses an encoder–decoder architecture, which enables text generation. However, the `facebook/bart-base` model is not trained as a causal language model. Although it can generate tokens due to the presence of a decoder, the lack of causal language modeling training results in poor-quality and incoherent text generation.\n"
      ],
      "metadata": {
        "id": "kKlq90dfPNK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2: Masked Language Modeling (Fill-Mask)\n",
        "\n",
        "### Task\n",
        "Predict the missing word in the sentence:\n",
        "\n",
        "> **\"The goal of Generative AI is to [MASK] new content.\"**\n",
        "\n",
        "### Expected Behavior\n",
        "- BERT and RoBERTa should perform well due to MLM training.\n",
        "- BART may behave inconsistently as MLM is not its primary training objective.\n"
      ],
      "metadata": {
        "id": "fAww1XjlPWuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masked_sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        fill_mask = pipeline(\"fill-mask\", model=model)\n",
        "        output = fill_mask(masked_sentence)\n",
        "        print(output[:3])\n",
        "    except Exception as e:\n",
        "        print(\"FAILED:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLdASp1XQBsE",
        "outputId": "70c325b6-955f-4c4f-cb65-d756e66bccf3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: BERT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.5396937131881714, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575705468654633, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.05405480042099953, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}]\n",
            "\n",
            "Model: RoBERTa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAILED: No mask_token (<mask>) found on the input\n",
            "\n",
            "Model: BART\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAILED: No mask_token (<mask>) found on the input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations (Experiment 2 – Masked Language Modeling)\n",
        "\n",
        "- **BERT** successfully predicted contextually appropriate words such as \"create\", \"generate\", and \"produce\" for the masked token with high confidence.\n",
        "- **RoBERTa** failed to execute the task and raised an error indicating that no `<mask>` token was found in the input sentence.\n",
        "- **BART** also failed with a similar error due to incompatible mask token usage.\n",
        "\n",
        "### Explanation\n",
        "\n",
        "BERT is explicitly trained using Masked Language Modeling (MLM), where tokens are replaced with the `[MASK]` symbol during training. As a result, it performs very well on fill-mask tasks using this format.\n",
        "\n",
        "RoBERTa and BART follow different tokenization and training conventions and expect the `<mask>` token instead of `[MASK]`. Since the input sentence did not match their expected mask format, both models failed to process the task. This highlights the importance of model-specific input requirements, even for similar NLP tasks.\n"
      ],
      "metadata": {
        "id": "wcdlJC6-QYea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 3: Question Answering\n",
        "\n",
        "### Question\n",
        "> **\"What are the risks?\"**\n",
        "\n",
        "### Context\n",
        "> **\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"**\n",
        "\n",
        "### Expected Behavior\n",
        "Since these are base models and not fine-tuned on QA datasets like SQuAD, outputs may be incomplete or inconsistent.\n"
      ],
      "metadata": {
        "id": "AcTMgcx3Qik9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are the risks?\"\n",
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        qa = pipeline(\"question-answering\", model=model)\n",
        "        output = qa(question=question, context=context)\n",
        "        print(output)\n",
        "    except Exception as e:\n",
        "        print(\"FAILED:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81OsYsXvQngN",
        "outputId": "4fc16225-89aa-4c19-de44-8e3c6954612e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: BERT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.004681949038058519, 'start': 20, 'end': 31, 'answer': 'significant'}\n",
            "\n",
            "Model: RoBERTa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.004456312395632267, 'start': 32, 'end': 81, 'answer': 'risks such as hallucinations, bias, and deepfakes'}\n",
            "\n",
            "Model: BART\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.0396338626742363, 'start': 0, 'end': 10, 'answer': 'Generative'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations (Experiment 3 – Question Answering)\n",
        "\n",
        "- **BERT** returned an extremely short and incomplete answer (\"significant\") with a very low confidence score.\n",
        "- **RoBERTa** extracted a longer and more relevant span mentioning \"risks such as hallucinations, bias, and deepfakes\", but the confidence score remained very low.\n",
        "- **BART** produced an incorrect and incomplete answer (\"Generative\"), failing to capture the actual risks described in the context.\n",
        "\n",
        "### Explanation\n",
        "\n",
        "All three models used in this experiment are base models and are not fine-tuned on Question Answering datasets such as SQuAD. As a result, the question-answering heads were randomly initialized, leading to unreliable and inconsistent outputs.\n",
        "\n",
        "Although RoBERTa extracted a more relevant text span compared to BERT and BART, the low confidence scores across all models indicate that pretraining alone is insufficient for accurate question answering. This experiment highlights the importance of task-specific fine-tuning for reliable QA performance.\n"
      ],
      "metadata": {
        "id": "Sbqpy15-Q3gP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Observation Table\n",
        "\n",
        "| Task | Model | Classification (Success / Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "|-----|------|-----------------------------------|----------------------------------------|---------------------------------------------|\n",
        "| Text Generation | BERT | Failure | Generated repetitive punctuation (dots) and failed to produce meaningful text. | BERT is an encoder-only model and is not trained for autoregressive next-token generation. |\n",
        "| Text Generation | RoBERTa | Failure | Repeated the input prompt without generating any continuation. | RoBERTa is also encoder-only and lacks a decoder for sequential text generation. |\n",
        "| Text Generation | BART | Partial Success | Generated a long continuation, but the output was noisy and incoherent. | BART has an encoder–decoder architecture enabling generation, but `bart-base` is not trained as a causal language model. |\n",
        "| Fill-Mask | BERT | Success | Correctly predicted words like \"create\", \"generate\", and \"produce\" with high confidence. | BERT is trained using Masked Language Modeling (MLM), making it well-suited for fill-mask tasks. |\n",
        "| Fill-Mask | RoBERTa | Failure | Failed with an error indicating that no `<mask>` token was found in the input. | RoBERTa expects the `<mask>` token instead of `[MASK]`, highlighting model-specific input requirements. |\n",
        "| Fill-Mask | BART | Failure | Failed with a similar mask token error and did not produce predictions. | BART is not designed for classic MLM tasks and expects different masking formats. |\n",
        "| Question Answering | BERT | Failure | Returned an extremely short and incomplete answer (\"significant\") with very low confidence. | The model is not fine-tuned on QA datasets, resulting in unreliable predictions. |\n",
        "| Question Answering | RoBERTa | Partial | Extracted a relevant answer span mentioning hallucinations, bias, and deepfakes, but with very low confidence. | Although RoBERTa has strong language understanding, it lacks QA-specific fine-tuning. |\n",
        "| Question Answering | BART | Failure | Returned an incorrect and incomplete answer (\"Generative\"). | BART requires task-specific QA fine-tuning despite having an encoder–decoder architecture. |\n"
      ],
      "metadata": {
        "id": "rhBh3jKnRIFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Insight\n",
        "\n",
        "This benchmark demonstrates that transformer models are highly task-dependent.\n",
        "Model architecture and training objectives strongly influence performance, and pretraining alone is insufficient for reliable task execution without proper fine-tuning.\n"
      ],
      "metadata": {
        "id": "Du02O2PWRR4c"
      }
    }
  ]
}